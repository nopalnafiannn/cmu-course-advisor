# Document Metadata

**Extraction Date**: 2025-03-21
**Key Concepts**: ###, Chapter, 2015, Textbook, Newcomer, Hatry, change, the course, grade, Chapter 1
**Document Length**: 2901 words

---

# 1
### H C , C M U

# EINZ OLLEGE ARNEGIE ELLON NIVERSITY

P E (90-823 S W)
# ROGRAM VALUATION ECTION

: 2024 (12 )
# FALL SEMESTER UNITS

### I NSTRUCTOR C ONTACT I N FORMATION :

Professor : Amanda Cash, DrPH, MPH Office Hours : by appointment
Class Time : Wednesday, 6pm-840pm E-mail : acash@andrew.cmu.edu ;
ajcash@gmail.com
Mobile/text – 202-375-0054
### I. C O URSE D E SCRIPTION :

Program evaluation provides an objective basis for decision making regarding program outcomes; it is
the systematic application of social science methods to assess needs for a program along with the
program's design, implementation, and outcomes. It results in (1) information regarding the program’s
merit, worth, or significance and (2) an accounting of the objective strengths and weaknesses of this
information.

Program evaluations are a critical component of determining program value. Social programs may
carry them out internally, or an external organization may carry out the evaluations. Evaluations and
their results may be highly politically charged or of interest only to direct stakeholders. There are
excellent program evaluations that have had tremendous impacts on programs and their target
populations. There are also poor or flawed program evaluations that provide weaker information than
could have been obtained, provide incorrect information, or are misleading.
### The program evaluation course is designed to:

● Provide the student with the theoretical, conceptual, methodological, and statistical tools of
program evaluation.
● Teach the student how to conduct basic and complex program evaluations, as well as how to
critique and monitor comprehensive program evaluations.
● Successful completion of this course will prepare students to be a contributing member of
teams that design and carry out program evaluations or that commission program evaluations
and make decisions based upon their results.
### II. C OURSE O BJECTIVES :

### By the completion of this course, the graduate student will:

● Understand the purpose of program evaluation, the various types of program evaluation
methodologies, and how to use program evaluation methods to answer research questions
relevant to policy, practice, and programing.
● Develop a logic model to express a theory of change , and specify strategies to implement this
theory of change for systems transformation.
2
● Describe interventions in measurable terms that articulate a theory of change and specify
underlying assumptions that guide service delivery strategies designed to produce change
and/or outcomes.
● Learn about performance measurement and systematic tracking of outputs and outcomes.
● Understand quantitative and qualitative data collection techniques used in program evaluation,
as well as state-of-the-art techniques for data analysis and deducing study conclusions.
● Master common pitfalls to program evaluation studies and communication of findings,
including threats to internal and external validity.
● Evaluate the applicability of evaluation findings to policy-makers, practitioners, and program
managers in public service, as well as how to translate evaluation findings into policy and
program recommendations.
### III. C O URSE T E XT :

The textbook is available at the bookstore or via Amazon.com
● Wholey, J.S., Hatry, H.P., & Newcomer, K.E. (2015, 4 th edition). Handbook of Practical
Program Evaluation. Jossey-Bass, San Francisco, CA.
● O ptional. Springer, F.J., Hass, P.J., & Porowski, A. (2017). Applied Policy Research:
Concepts and Cases. Routledge, New York, NY.
### IV. R EQUIRED A DDITIONAL R EADINGS :

The additional required readings will be posted via Canvass.
● Worthen, B., Sanders, J., Fitzpatrick, J. (1997). Program evaluation, alternative approaches and
practical guidelines. Evaluation’s basic purpose, use, and conceptual distinctions (Chapter 1).

New York.
● Werner, A. (2004). A guide to implementation research. An introduction to implementation
research (Chapter 1). Urban Institute.
● W.K. Kellogg Foundation: Logic model development guide. Available at
http://www.wkkf.org/knowledge-center/resources/2006/02/wk-kellogg-foundation-logic-model
-development-guide.aspx
● Schorr, L., & Farrow, F. (2011). Expanding the evidence universe: Doing better by knowing
more. Center for the Study of Social Policy.
● Haskins, R., & Baron, J. (2011). Building the connection between policy and evidence. The
Obama evidenced-based initiatives. NESTA.
● Cartwright, N. (2007) Are RCTs the gold standard? Biosocieties, 11-20.
● Heinrich, C., Maffioli, A., & Vazquez, G. (2010). A primer for applying propensity-score
matching . Inter-American Development Bank.
● Hunter, D. (2006). Daniel and the rhinoceros. Evaluation and Programming Planning, 29,
180-185.
● Reichardt, C. S. (2009). Quasi-experimental design. The SAGE handbook of quantitative
methods in psychology, 46, 71.
● Eccles, M., Grimshaw, J., Campbell, M., & Ramsay, C. (2003). Research designs for studies
evaluating the effectiveness of change and improvement strategies. Quality and Safety in
Health Care, 12(1), 47-52.
● Shrank, W. (2013). The Center for Medicare and Medicaid Innovation Blueprint for Rapid
Cycle Evaluation of New Care and Payment Models. Health Affairs, 32, 4, 1-6.
3
● Cody, S., & Asher, A. (2014). Smarter, Better, Faster: The Potential for Predictive Analytics
and Rapid-Cycle Evaluation to Improve Program Development and Outcomes, Hamilton
Project, Brookings Institution.
● Rangan, V. & Chase, L. The payoff of pay for success. Stanford Social Innovation Review,
Fall 2015.
● Thoemmes, Felix. "Propensity score matching in SPSS." arXiv preprint arXiv:1201.6385
(2012).
● Thoemmes, Felix J., and Eun Sook Kim. "A systematic review of propensity score methods in
the social sciences." Multivariate Behavioral Research 46.1 (2011): 90-118.

Other Resources
● Checklist For Reviewing a Randomized Controlled Trial of a Social Program or Project, To
Assess Whether It Produced Valid Evidence, Coalition for Evidence-Based Policy, 2010.
http://coalition4evidence.org/wp-content/uploads/2010/02/Checklist-For-Reviewing-a-RCT-Jan
10.pdf
● Key Items to Get Right When Conducting Randomized Controlled Trials of Social Programs,
LJAF Evidence-Based Policy team, 2016.
http://www.arnoldfoundation.org/wp-content/uploads/Key-Items-to-Get-Right-in-an-RCT.pdf
● Rigorous Program Evaluations on a Budget: How Low-Cost Randomized Controlled Trials Are
Possible in Many Areas of Social Policy, 2012 (.pdf, 6
pages). http://coalition4evidence.org/wp-content/uploads/2012/03/Rigorous-Program-Evaluatio
ns-on-a-Budget-March-2012.pdf
● How to Read Research Findings to Distinguish Evidence-Based Programs from Everything
Else. http://coalition4evidence.org/help-desk/workshop/
### V. P R EREQUISITES

Students should complete the first year of the MS DC core curriculum before enrolling in this class.
### VI. Y OUR R O LE :

### We are partners in this learning experience. I EXPECT YOU TO:

✔ Attend class and constructively participate in it
✔ Read materials for each week
✔ Contribute to and take responsibility for assignments
✔ Prepare for exams
✔ Conduct your learning with academic integrity
✔ Be aware of and be proactive about your own learning style and time management
4
✔ Communicate with me as soon as you have any issues
### VII. A T TENDANCE P OLICY :

Class participation is graded (it is worth 10% of the total course grade – see grading section of
syllabus). Students are expected to attend all classes. However, there can be unforeseen circumstances
and emergencies that arise. Students may be granted one excused absence for the course which could
include an illness or personal emergency (you need to contact me within 1-2 days of missing class if
not sooner in order to be excused) or an apprenticeship-related travel/opportunity that is worked out
with me in advance of the missed class. After the one excused absence, or for any unexcused
absences, the student can choose to submit a make-up paper (due within two weeks of the missed
class) OR receive a “0” for their participation grade for each missed class which will factor into the
student’s final grade for the course. The student should contact me to work out the topic for the paper.

Please note that even if a student misses a class (whether excused or unexcused), assignments due for
that day must still be completed and handed in. Under certain circumstances, such as illness of the
student, the instructor may grant extensions to due dates.
### VIII. C OURSE P OLICIES :

Referencing is expected whenever quoting or otherwise using others’ work (such as paraphrasing or
employing key ideas). Standard APA style will be used for in-text citations and for references. Given
the availability of information on the Web, it is often difficult to evaluate the quality of online sources.

It is expected that students will pay attention to the domain, sponsor, author’s background, and date of
information on websites used and will cite all information obtained from websites (see APA Manual 5 th
edition, for how to reference sources from electronic media). In general, on-line sources should be
from refereed journals unless you find an exceptionally well documented website related to your
topic/research.

Academic conduct . Students are subject to Carnegie Mellon University’s policies on academic
integrity (http://www.cmu.edu/academic-integrity/plagiarism/index.html). Plagiarism is a serious
offense that will result in the student failing the course. Note that all academic integrity violations will
be reported to the Associate Dean. Additional penalties may be imposed. Plagiarism includes:
● Presenting another writer’s work as your own;
● Cutting and pasting content verbatim without using quotation marks to indicate a direct quote;
● Inserting a direct quote or paraphrasing content without citing the source in-text using
footnotes, endnotes, or parenthetical citations with a corresponding Works Cited, References, or
Notes page – in a manner consistent with an APA, MLA, or Chicago style guide;
● Providing incomplete or incorrect information about the source cited;
● Relying on artificial intelligence tools to produce assignments;
● Over-relying on templates or other writers’ phrasing.

Also, submitting work written for another course is not acceptable; consequently, a failing grade will
be issued for that assignment.

In this class, any use of generative AI for any graded course assignment is prohibited. Passing off any generated
content as your own (for example - cutting and pasting content into written assignments, or paraphrasing AI
5
content) constitutes an academic integrity violation. If you have questions about using generative AI in this
course please talk to me first before doing so.
### IX. G RADING S CALE :

### The grading scale will be:

A+ 100-98 B+ 89-87 C 70-79
A 97-94 B 86-83 F <70
A- 93-90 B- 80-82
### X. A S SIGNMENTS AND C A PSTONE P ROJECT :

A. Homework Assignments. There will be 4-6 short homework assignments given in class (due the
following week). The homework assignments will be spread across the semester as we cover specific
topics.

DUE : Various/TBD (25% of grade)
B. Quiz. The quiz covers materials to date in course (Section 1 of course).

DUE : October 2nd (20% of grade)
C. Concept Paper: Policy and Evidence. Select option #1 or #2. Develop an approximately 5-page
paper on one of the following options:
### Option #1. Select a prominent program evaluation study in the published literature:

## 1. Summarize the research questions, study design, and methods

## 2. Critique the study

o What were the strengths and weaknesses?
o What would you change to improve the study?
## 3. What are the study implications?

Option #2. Develop a paper of approximately 5-8 pages in length based on an emerging topic in the
field of evaluation (e.g., equity-focused program evaluation; rapid cycle evaluation, AI in evaluation).
## 1. Present a literature review on the initiative, including key issues.

## 2. Discuss the merits and challenges of the framework.

## 3. Discuss the long-term implications of the initiative and its potential to impact public policy,

including programmatic efforts.

In a 10-minute presentation, share with the class your concept paper based on the evidenced-based
concept paper developed in Assignment C above.

DUE : November 6 (20% of grade)
6
D. Capstone/Final Project and Presentation. The capstone project and presentation covers all
materials in the course and the expectation is to take what you are learning and apply it to a real-world
scenario. You will be expected to work in teams, and at the end of the semester your team will be
expected to present the final technical proposal to the professor, and each team will be expected to
present to the class on the final day of class. Each person in the team will be required to present some
aspect of the project in the final presentation. A separate document will be shared outlining the
requirements of the project.

DUE : Dec 11 (30% of grade)
E. Class Participation.

DUE : N/A (10% of grade)
### XI. C OURSE O UTLINE :

Please find below the course outline. It is expected that you read each assignment prior to coming to
class. Throughout the course, we will use a mix of lecture, in-class group work, and discussion.
### WEEK DISCUSSION TOPIC READINGS, ACTIVITIES & ASSIGNMENTS

## Section #1: PROGRAM EVALUATION: OVERVIEW & PLANNING

1 ● Introductions to course ● Course Textbook, Chapter 1, Planning and designing
8/28/24 ● Syllabus useful evaluations (Newcomer, Hatry, Wholey, 2015)
● Overview of program ● Program evaluation, alternative approaches and
evaluation practical guidelines, Chapter 1, Evaluation’s basic
● Overview of the final project purpose, use, and conceptual distinctions (Worthen,
and expectations - Developing Sanders, Fitzpatrick, 1997)
and bidding on an RFP for ● Homework Assignment 1 – see slides
evaluation services
2 ● Overview of program eval. ● Course Textbook, Chapter 2, Analyzing and
9/4/24 (cont.) engaging stakeholders (Bryson, Quinn Patton, 2015)
● Stakeholder engagement ● A guide to implementation research, Chapter 1, an
● Implementation research introduction to implementation research (Werner,
● Types of program evaluation 2004)
studies (e.g., formative, ● In Class Assignment – see Capstone Project
summative) Outline
3 ● Theory of Change ● Course Textbook, Chapter 3, Using logic models
9/11/24 ● Logic models (McLaughlin, Jordan, 2015)
● Outputs vs. Outcomes ● W.K. Kellogg Foundation: Logic model development
guide (2004)
### ● In Class Assignment – see Capstone Project

Outline
4 ● Performance measurement ● Course Textbook, Chapter 5, Performance
9/18/24 Measurement: Monitoring program outcomes
(Poister, 2015)
7
● Difference between ● Milestone due for Capstone : draft methods and/or
performance measurement and design outline, research questions, and draft logic
model should be ready and be prepared to discuss in
evaluation
class.
## Section #2: PROGRAM EVALUATION: TYPES OF METHODOLOGICAL DESIGNS

5 ● RCT designs (causal designs) ● Course Textbook, Chapter 7, Randomized Controlled
9/25/24 ● Baseline equivalency Trials and Nonrandomized Designs (Torgerson,
Torgerson, & Taylor, 2015)
Virtual
● Course Textbook, Chapter 6, Comparison group
Class
designs (Henry, 2015)
6 ● Quasi-experimental designs ● Assignment #B Due: QUIZ [ C overs Section
10/2/24 ● Propensity score matching 1 of Course]
● Threats to internal and external ● Reichardt, C. S. (2009). Quasi-experimental design.

The SAGE handbook of quantitative methods in
validity
psychology, 46, 71.
● Heinrich, C., Maffioli, A., & Vazquez, G. (2010). A
primer for applying propensity-score matching .

Inter-American Development Bank.
● Thoemmes, Felix. "Propensity score matching in
SPSS." arXiv preprint arXiv:1201.6385 (2012).
### ● Thoemmes, Felix J., and Eun Sook Kim. "A

systematic review of propensity score methods in the
social sciences." Multivariate Behavioral Research
46.1 (2011): 90-118.
### ● Eccles, M., Grimshaw, J., Campbell, M., & Ramsay,

C. (2003). Research designs for studies evaluating
the effectiveness of change and improvement
strategies. Quality and Safety in Health Care, 12(1),
47-52.
7 ● Multisite evaluations ● Course Textbook, Chapter 10, Designing, managing,
10/9/24 ● Rapid cycle evaluations and analyzing multisite evaluations (Rog, 2015)
● Predictive analytics ● Shrank, W. (2013). The Center for Medicare and
### Medicaid Innovation Blueprint for Rapid Cycle

Evaluation of New Care and Payment Models.

Health Affairs, 32, 4, 1-6.
● Cody, S., & Asher, A. (2014). Smarter, Better,
### Faster: The Potential for Predictive Analytics and

### Rapid-Cycle Evaluation to Improve Program

### Development and Outcomes, Hamilton Project,

Brookings Institution.
8 FALL BREAK Work on your concept paper on Evidence
10/16/24
9 ● Evaluation design debates ● Cartwright, N. (2007) Are RCTs the gold standard?
10/23/24 ● The Evidence Act Biosocieties, 11-20.
● Hunter, D. (2006). Daniel and the rhinoceros.

Guest
Evaluation and Programming Planning, 29, 180-185.

Lecturer
10 ● Tiered-evidence programs ● Building the connection between policy and
10/30/24 ● Pay for success evidence. The Obama evidenced-based initiatives.

NESTA. Haskins, R., & Baron, J. (2011)
● Evidenced based programming
● Rangan, V. & Chase, L. The payoff of pay for
success. Stanford Social Innovation Review, Fall
2015.
8
### ● Expanding the Evidence Universe: Doing Better by

Knowing More (Schorr & Farrow, 2011)
### 11 N/A Assignment #C Due: Concept Paper on Evidence

11/6/24 and Class Presentations
## Section #3: DATA COLLECTION & DATA ANALYSIS

12 ● Study recruitment and retention ● Course Textbook, Chapter 9, Recruitment and
11/13/24 ● Incentives retention of study participants (Cook, Godiwalla,
Brooks, Powers, & John, 2015)
● Survey methods
● Course Textbook, Chapter 14, Using surveys
(Newcomer, Triplett, 2015)
13 ● Data sources (agency records) ● Course Textbook Chapter 13, Using agency records
11/20/24 ● Field data collection (Hatry, 2015)
### ● Course Textbook, Chapter 17, Collecting Data in the

● Qualitative techniques
Field (Nightingale, Rossman, 2015)
● Focus groups
● Course Textbook, Chapter 20, Focus group
● Interviews interviewing (Krueger, Casey, 2015)
### ● Stories/anecdotes ● Course Textbook, Chapter 19, Conducting

semi-structured interviews (Adams, 2015)
No Class No Class – Thanksgiving Break No Class – Thanksgiving Break
11/27/24
## Section #4: USE OF EVALUATION

14 ● Content analysis/coding ● Course Textbook, Chapter 22, qualitative analysis
12/4/24 ● Inferential statistics (t-test, (Goodrick, Rogers, 2015)
● Course Textbook, Chapter 23, Using statistics in
ANOVA, regression, etc.)
evaluation (Newcomer, Conger, 2015)
● Using statistics in evaluation
### ● Course Textbook, Chapter 29, Contracting for

● Evaluation firms evaluation products and services (Bell, 2015)
● Careers in evaluation ● Course Textbook, Chapter 26, Pitfalls in evaluation
● Evaluation pitfalls (Hatry, Newcomer, 2015)
● Course Review
### 15 N/A Assignment #D Due : Final Project Presentations

12/11/24
– Lightening Rounds
8